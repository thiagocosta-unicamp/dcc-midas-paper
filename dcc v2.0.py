import pandas as pd
import numpy as np
import statsmodels.api as sm
import plotly.express as px
import plotly.figure_factory as ff
from arch import arch_model  # Importing GARCH model implementation from arch package
from ipywidgets import HBox, VBox, Dropdown, Output
from scipy.optimize import fmin, minimize  # For optimization (minimization)
from scipy.stats import t, norm  # For statistical distributions
from math import inf  # For infinity value
from IPython.display import display  # For displaying outputs in Jupyter
import yfinance as yf  # For downloading stock market data from Yahoo Finance
import datetime  # For handling date and time
import streamlit as st  # For creating web applications (interactive dashboards)
import matplotlib.pyplot as plt  # For plotting graphs
import pandas_datareader.data as web  # For downloading economic data (like CPI) from FRED
import numdifftools as ndt 

#%%Functions

def garch_t_to_u(rets, res):
    """
    This function transforms returns adjusted by a GARCH model into uniformly distributed data.

    Steps performed:
    1. Subtracts the estimated mean (mu) from observed returns to get adjusted returns.
    2. Divides the adjusted returns by the estimated conditional volatility to standardize them.
    3. Uses the CDF of the t-distribution to convert standardized residuals into uniformly distributed data.

    Args:
    rets (ndarray): Array of returns.
    res (GARCHResults): Result from fitting the GARCH model.

    Returns:
    udata (ndarray): Uniformly distributed data after transformation.
    """
    # Extract the mean (mu) from the GARCH model results
    mu = res.params['mu']
    
    # Extract the degrees of freedom (nu) for the t-distribution from the GARCH results
    nu = res.params['nu']
    
    # Adjust the returns by subtracting the mean (mu)
    est_r = rets - mu

    # Extract the conditional volatility (h) from the GARCH results
    h = res.conditional_volatility
   
    # Standardize the returns by dividing by the conditional volatility
    std_res = est_r / h
    
    # Transform the standardized residuals into uniformly distributed data using the CDF of the t-distribution
    udata = t.cdf(std_res, nu)
    
    return udata


def run_garch_on_return(rets, udata_list, model_parameters):
    """
    Fit a GARCH model to each series of returns in the 'rets' dictionary.

    Args:
    rets (dict): Dictionary of return series (e.g., S&P500, Bonds).
    udata_list (list): List to store the uniformly distributed data.
    model_parameters (dict): Dictionary to store the results of the fitted GARCH models.

    Returns:
    udata_list (list): List containing the uniformly distributed data from each series.
    model_parameters (dict): Dictionary containing the fitted GARCH model results.
    """
    for x in rets:
        # Fit a GARCH(1,1) model with t-distribution to the return series
        am = arch_model(rets[x], vol='GARCH', p=1, q=1, dist='t')
        
        # Shorten the series name for the key in the dictionary
        short_name = x.split()[0]

        # Fit the model and store the result in model_parameters
        model_parameters[short_name] = am.fit(disp='off')

        # Convert the fitted returns to uniform data using the garch_t_to_u function
        udata = garch_t_to_u(rets[x], model_parameters[short_name])
        
        # Append the uniform data to the list
        udata_list.append(udata)

    # Return the updated list of uniform data and model parameters
    return udata_list, model_parameters


def plot_conditional_volatility(rets, model_parameters):
    """
    Plot the conditional volatility of the GARCH model for each return series.

    Args:
    rets (dict): Dictionary of return series.
    model_parameters (dict): Dictionary containing the fitted GARCH models.
    """
    for x in rets:
        # Extract the short name for the series
        short_name = x.split()[0]
        
        # Retrieve the results of the fitted model for that series
        res = model_parameters[short_name]
        
        # Extract the conditional volatility from the model results
        volatility = res.conditional_volatility
                  
        # Plot the volatility
        plt.figure(figsize=(12, 6))
        plt.plot(volatility, label='Conditional Volatility', color='blue')
        plt.title(f'Conditional Volatility for {short_name}')
        plt.xlabel('Date')
        plt.ylabel('Conditional Volatility (%)')
        plt.legend()
        plt.grid()
        plt.show()


def loglikelihood_garch(params, udata_list, K, MV, lambda_param):
    """
    Calculate the log-likelihood for a GARCH(1,1) model.

    Args:
    params (ndarray): Model parameters [alpha, beta].
    udata_list (list): List of standardized residuals.
    K (int): Number of lags for macroeconomic variables.
    MV (ndarray): Macro variables.
    lambda_param (float): Decay parameter for exponential weights.

    Returns:
    float: Log-likelihood value.
    """
    # Assign alpha and beta from the parameters
    alpha, beta = params

    T = len(udata_list[0])
    q_11 = np.zeros(T)
    q_12 = np.zeros(T)
    q_22 = np.zeros(T)
    lt_rho = np.zeros(T)
    m1 = np.zeros(T)
    rho = np.zeros(T)
    
    z1 = udata_list[0]  # Residuals of S&P 500
    z2 = udata_list[1]  # Residuals of US Bond

    # Initialize weights for lagged realized correlations
    phi = np.zeros(K + 1)
    for i in range(1, K + 1):
        phi[i] = (lambda_param ** (i - 1))

    # Define long-term correlation calculation
    
    #MV = MV.values
    # for i in range(1, K + 1):
    #     m1 +=theta1 * phi[i] * MV[ : , i - 1]  
    # lt_rho = m1
    # lt_rho = (np.exp(2 * lt_rho) - 1) / (np.exp(2 * lt_rho) + 1)  # Fisher transformation

    lt_rho = 0.35  # This is hardcoded; in practice, it should be calculated

    q_11[0] = 1
    q_12[0] = (np.mean(z1 * z2) - np.mean(z1) * np.mean(z2)) / \
               (np.std(z1) * np.std(z2))
    q_22[0] = 1

    garchln = np.zeros(T)

    # Main loop for calculating log-likelihood based on GARCH model
    for i in range(1, T):
        q_11[i] = (1 - alpha - beta) * q_11[0] + alpha * z1[i - 1] ** 2 + beta * q_11[i - 1]
        q_22[i] = (1 - alpha - beta) * q_22[0] + alpha * z2[i - 1] ** 2 + beta * q_22[i - 1]
        q_12[i] = (1 - alpha - beta) * lt_rho + alpha * z1[i - 1] * z2[i] + beta * q_12[i - 1]
        
        rho[i] = q_12[i] / np.sqrt(q_11[i] * q_22[i])
        
        # Calculate the log-likelihood
        garchln[i] = -np.log(1 - rho[i] ** 2) - (1 / (1 - rho[i] ** 2)) * (
            z1[i] ** 2 - 2 * rho[i] * z1[i] * z2[i] + z2[i] ** 2
        )

    return np.sum(garchln[1:])  # Return the sum of log-likelihood values, excluding the first observation


def dcc_rc_x(params, udata_list, K, MV, lambda_param):
    """
    Calculate the log-likelihood for a GARCH(1,1) model with an additional term for macroeconomic variables.

    Similar to the previous log-likelihood function but includes additional terms for lagged correlations.
    """
    # Similar logic as loglikelihood_garch, with additional steps for macroeconomic variables (MV)
    alpha, beta = params

    T = len(udata_list[0])
    q_11 = np.zeros(T)
    q_12 = np.zeros(T)
    q_22 = np.zeros(T)
    lt_rho = np.zeros(T)
    m1 = np.zeros(T)
    rho = np.zeros(T)

    z1 = udata_list[0]
    z2 = udata_list[1]

    phi = np.zeros(K + 1)
    for i in range(1, K + 1):
        phi[i] = (lambda_param ** (i - 1))

    lt_rho = 0.35
    q_11[0] = 1
    q_12[0] = (np.mean(z1 * z2) - np.mean(z1) * np.mean(z2)) / \
               (np.std(z1) * np.std(z2))
    q_22[0] = 1

    garchln = np.zeros(T)

    for i in range(1, T):
        q_11[i] = (1 - alpha - beta) * q_11[0] + alpha * z1[i - 1] ** 2 + beta * q_11[i - 1]
        q_22[i] = (1 - alpha - beta) * q_22[0] + alpha * z2[i - 1] ** 2 + beta * q_22[i - 1]
        q_12[i] = (1 - alpha - beta) * lt_rho + alpha * z1[i - 1] * z2[i] + beta * q_12[i - 1]
        
        rho[i] = q_12[i] / np.sqrt(q_11[i] * q_22[i])
        
        garchln[i] = -np.log(1 - rho[i] ** 2) - (1 / (1 - rho[i] ** 2)) * (
            z1[i] ** 2 - 2 * rho[i] * z1[i] * z2[i] + z2[i] ** 2
        )

    return np.sum(garchln[1:]), q_11, q_22, q_12, rho, garchln


def calculate_criteria(log_likelihood, num_params, T):
    """
    Calculate AIC, BIC, and HQC given the log-likelihood, number of parameters, and number of observations.
    """
    AIC = -2 * log_likelihood + 2 * num_params
    BIC = -2 * log_likelihood + np.log(T) * num_params
    HQC = -2 * log_likelihood + 2 * np.log(np.log(T)) * num_params
    
    return {'AIC': AIC, 'BIC': BIC, 'HQC': HQC}

def QMLE(params, udata_list, K, MV, lambda_param, hessian, log_likelihood):
    """
    Estimate the parameters using QMLE (Quasi Maximum Likelihood Estimation), returning the log-likelihood, errors, and p-values.
    
    Args:
    params (np.ndarray): The parameters of the model [alpha, beta].
    udata_list (list): The list of standardized residuals.
    K (int): Number of lags for macro variables.
    MV (np.ndarray): Macro variables data.
    lambda_param (float): Exponential weight decay parameter.
    hessian (np.ndarray): The Hessian matrix (second derivative of log-likelihood).
    log_likelihood (float): The log-likelihood value calculated from the model.
    
    Returns:
    dict: A dictionary containing the log-likelihood, parameter standard errors, and p-values.
    """

    # Number of parameters in the model
    num_params = len(params)
    
    # Number of observations (T)
    T = len(udata_list[0])
    
    # Calculate information criteria (AIC, BIC, HQC)
    criteria = calculate_criteria(log_likelihood, num_params, T)
        
    # Calculate standard errors (square root of diagonal elements of the inverse Hessian)
    hessian_inv = np.linalg.inv(hessian)
    std_errors = np.sqrt(np.diagonal(hessian_inv))
    
    # Calculate p-values using the normal distribution and standard errors
    p_values = 2 * (1 - norm.cdf(np.abs(params / std_errors)))  # Calculate p-values based on Z-scores
    
    return {
        'log_likelihood': log_likelihood,
        'AIC': criteria['AIC'],
        'BIC': criteria['BIC'],
        'HQC': criteria['HQC'],
        'params': params,
        'std_errors': std_errors,
        'p_values': p_values,  # Include gradient for transparency
        'hessian': hessian     # Include Hessian for transparency
    }

#%% Analysis

# Starting point of the analysis
start = datetime.datetime(2000, 1, 1)
end = datetime.datetime(2020, 12, 30)

# Define tickers for S&P 500 and US Treasury Bond yield
tickers = ['^GSPC', '^TNX']

# Download adjusted close prices for the specified tickers
close_prices = yf.download(tickers, start=start, end=end)['Adj Close']

# Calculate daily returns from the adjusted close prices
rets = ((close_prices / close_prices.shift(1)) - 1).dropna() * 100  # Returns in percentage

# Download CPI data from FRED (Federal Reserve Economic Data)
cpi_data = web.DataReader('CPIAUCNS', 'fred', start, end)
cpi_data = np.log(cpi_data).diff()  # Calculate log differences to obtain inflation rate
cpi_data = cpi_data.dropna()  # Drop missing values

# Define the macroeconomic variables (CPI data)
MV = cpi_data

# Initialize dictionaries for storing model parameters and uniform data
model_parameters = {}
udata_list = []

# Run GARCH model on the returns data
udata_list, model_parameters = run_garch_on_return(rets, udata_list, model_parameters)

# Plot the conditional volatility of the fitted GARCH models
plot_conditional_volatility(rets, model_parameters)

# Optimize the log-likelihood to find the best-fitting parameters
initial_params = np.array([0.05, 0.94])  # Initial guess for parameters [alpha, beta]
lambda_param = 0.96  # Decay parameter for exponential smoothing
K = 48  # Number of lags for the macroeconomic variables

# Define constraints and bounds for the optimization
cons = ({'type': 'ineq', 'fun': lambda x: -x[0] - x[1] + 1})  # Constraints on alpha and beta
bnds = ((0, 0.5), (0, 0.9997))  # Bounds for alpha and beta parameters

# Minimize the log-likelihood function to estimate the optimal parameters
opt_out = minimize(loglikelihood_garch, initial_params, args=(udata_list, K, MV, lambda_param), bounds=bnds, constraints=cons, method = 'L-BFGS-B')

# Extract optimized parameters and log-likelihood
opt_params = opt_out.x
opt_loglikehood = opt_out.fun
opt_hessian = opt_out.hess_inv.todense()

# Get the log-likelihood, q_11, q_22, q_12, and rho from the dcc_rc_x function
log_likelihood, q_11, q_22, q_12, rho, garch_ln_matrix = dcc_rc_x(opt_params, udata_list, K, MV, lambda_param)

# Calculate the information criteria (AIC, BIC, HQC) based on the optimized log-likelihood
information_criteria = calculate_criteria(log_likelihood=opt_loglikehood, num_params=len(opt_params), T=len(udata_list[0]))

# Estimate the model parameters using QMLE (Quasi Maximum Likelihood Estimation)
results = QMLE(params=opt_params, udata_list=udata_list, K=K, MV=MV, lambda_param=lambda_param, hessian=opt_hessian, log_likelihood=log_likelihood)

# Print the results: log-likelihood, AIC, BIC, HQC, parameters, standard errors, and p-values
print("Log-Likelihood:", results['log_likelihood'])
print("AIC:", results['AIC'])
print("BIC:", results['BIC'])
print("HQC:", results['HQC'])
print("Parameters:", results['params'])
print("Standard Errors:", results['std_errors'])
print("P-values:", results['p_values'])
